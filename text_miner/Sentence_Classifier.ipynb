{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单晶合金"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import BertTokenizerFast\n",
    "import psie\n",
    "\n",
    "from seqeval.metrics import classification_report\n",
    "import os\n",
    "from torch import cuda\n",
    "import nltk\n",
    "# nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "import re\n",
    "from pymatgen.core import Composition\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"PyTorch is using GPU\")\n",
    "else:\n",
    "    print(\"PyTorch is using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_VERSION = r'/pretrained_models/m3rg-iitd/matscibert'\n",
    "MAX_LEN = 256\n",
    "extr_target = 'Solvus'\n",
    "MAIN_DIR = os.getcwd()\n",
    "MODEL_DIR = os.path.join(\"models\", extr_target, \"classifier\")\n",
    "CORPUS = os.path.join(\"corpus\", extr_target, \"classifier/corpus_sentences.json\")\n",
    "OUTPUT=\"relevant_sentences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('json', data_files=os.path.join(MAIN_DIR, CORPUS))[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from psie import classifier\n",
    "\n",
    "# 重新加载模块\n",
    "importlib.reload(classifier)\n",
    "from psie import classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = psie.classifier.BertClassifier()\n",
    "model.to(device)   # 将模型移动到指定GPU上进行计算\n",
    "\n",
    "# # 加载模型参数。strict=False表示只加载部分权重\n",
    "# model.load_state_dict(torch.load(r'./models/Tc/classifier.pt'),strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建实例，对文本进行编码\n",
    "tokenizer = BertTokenizerFast.from_pretrained(BERT_VERSION)\n",
    "\n",
    "def encode(paper):\n",
    "  return tokenizer(paper[\"sentence\"], truncation=True, max_length=MAX_LEN, padding=\"max_length\")\n",
    "\n",
    "dataset = dataset.map(encode, batched=True)\n",
    "dataset.set_format(type=\"torch\", columns=[\"source\", \"sentence\", \"input_ids\", \"attention_mask\",\n",
    "                                         \"isrelevant\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset[0]['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "划分数据集\n",
    "* train_size = 0.6  # 训练集比例\n",
    "* val_size = 0.2  # 验证集比例\n",
    "* test_size =0.2  # 测试集比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val = dataset.train_test_split(\n",
    "    test_size=0.4,shuffle=True,seed=666)\n",
    "train_dataset = train_val['train']\n",
    "test_dataset = train_val['test']\n",
    "test_val = test_dataset.train_test_split(test_size=0.5,shuffle=True,seed=666)\n",
    "val_dataset = test_val['train']\n",
    "test_dataset = test_val['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载编码后的数据集\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "# 优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "max_norm = 1.0  # 梯度裁剪的最大范数\n",
    "\n",
    "# 加权损失\n",
    "class_weights = torch.tensor([1.0, 303.0/50.0],dtype=torch.float32).to(device)  # 将权重列表转换为张量，并移动到设备上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = Num_Epochs\n",
    "tr_Loss_list = []\n",
    "tr_Acc_list = []\n",
    "val_Loss_list = []\n",
    "val_Acc_list = []\n",
    "val_f1_list = []\n",
    "val_recall_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    epoch_loss_tr, tr_accuracy, val_loss, val_accuracy, val_f1, val_recall = model.finetuning(train_loader,val_loader,\n",
    "                        device, max_norm, optimizer,weight=class_weights)\n",
    "    tr_Loss_list.append(epoch_loss_tr)\n",
    "    tr_Acc_list.append(tr_accuracy)\n",
    "    val_Loss_list.append(val_loss)\n",
    "    val_Acc_list.append(val_accuracy)\n",
    "    val_f1_list.append(val_f1)\n",
    "    val_recall_list.append(val_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型参数\n",
    "torch.save(model.state_dict(), './classifier.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = psie.classifier.BertClassifier()\n",
    "# 加载预训练的模型参数。strict=False表示只加载部分权重\n",
    "model.load_state_dict(torch.load('./classifier.pt'),strict=False)\n",
    "model.to(device)   # 将模型移动到指定GPU上进行计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_tensor, preds = model.testLabeledData(test_loader, device)\n",
    "# 将张量转换为标准的 Python 列表\n",
    "labels = [label.item() for label in labels_tensor]\n",
    "predictions = []\n",
    "for i in range(len(preds)):\n",
    "  predictions.append(np.argmax(preds[i].cpu().numpy()))   # 返回最大值的索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels_tensor[0])\n",
    "print(preds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "model = psie.classifier.BertClassifier()\n",
    "# 加载预训练的模型参数。strict=False表示只加载部分权重\n",
    "model.load_state_dict(torch.load('./classifier.pt'),strict=False)\n",
    "model.to(device)   # 将模型移动到指定GPU上进行计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_filtered_sentences = {\"sentence\": [], \"source\": []}\n",
    "tokenizer = BertTokenizerFast.from_pretrained(BERT_VERSION)\n",
    "def encode(paper):\n",
    "  return tokenizer(paper[\"sentence\"], truncation=True, max_length=MAX_LEN, padding=\"max_length\")\n",
    "\n",
    "for filename in os.listdir(INPUT_DIR):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(INPUT_DIR, filename)\n",
    "        \n",
    "        # 导入数据集\n",
    "        dataset = load_dataset('json', data_files=file_path)['train']\n",
    "        dataset = dataset.map(encode, batched=True)\n",
    "        dataset.set_format(type=\"torch\", columns=[\"source\", \"sentence\", \"input_ids\", \"attention_mask\"])\n",
    "        # 加载编码后的数据集\n",
    "        dataset_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "        # 预测\n",
    "        pred = model.predict(dataset_loader, device)\n",
    "        predictions = []\n",
    "        for i in range(len(pred)):\n",
    "            predictions.append(np.argmax(pred[i].cpu().numpy())) \n",
    "  \n",
    "        filtered_sentences = {\"sentence\": [], \"source\":[]}\n",
    "        for i in range(len(predictions)):\n",
    "            if predictions[i] == 1:\n",
    "                filtered_sentences[\"sentence\"].append((dataset[i][\"sentence\"]))\n",
    "                filtered_sentences[\"source\"].append((dataset[i][\"source\"]))\n",
    "        \n",
    "        combined_filtered_sentences[\"sentence\"].extend(filtered_sentences[\"sentence\"])\n",
    "        combined_filtered_sentences[\"source\"].extend(filtered_sentences[\"source\"])\n",
    "        \n",
    "\n",
    "with open(OUTPUT_FILE, 'w', encoding='utf-8') as file:\n",
    "    json.dump(combined_filtered_sentences, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理单个文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "model = psie.classifier.BertClassifier()\n",
    "# 加载预训练的模型参数。strict=False表示只加载部分权重\n",
    "model.load_state_dict(torch.load('./classifier.pt'),strict=False)\n",
    "model.to(device)   # 将模型移动到指定GPU上进行计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('json', data_files=os.path.join(MAIN_DIR,'/results.json'))['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建实例，对文本进行编码\n",
    "tokenizer = BertTokenizerFast.from_pretrained(BERT_VERSION)\n",
    "\n",
    "def encode(paper):\n",
    "  return tokenizer(paper[\"sentence\"], truncation=True, max_length=MAX_LEN, padding=\"max_length\")\n",
    "\n",
    "\n",
    "dataset = dataset.map(encode, batched=True)\n",
    "dataset.set_format(type=\"torch\", columns=[\"source\", \"sentence\", \"input_ids\", \"attention_mask\"])\n",
    "# 加载编码后的数据集\n",
    "dataset_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(dataset_loader, device)\n",
    "predictions = []\n",
    "for i in range(len(pred)):\n",
    "  predictions.append(np.argmax(pred[i].cpu().numpy()))   # 返回最大值的索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentences = {\"sentence\": [], \"source\":[]}\n",
    "for i in range(len(predictions)):\n",
    "  if predictions[i] == 1:\n",
    "    filtered_sentences[\"sentence\"].append((dataset[i][\"sentence\"]))\n",
    "    filtered_sentences[\"source\"].append((dataset[i][\"source\"]))\n",
    "\n",
    "with open(os.path.join(MAIN_DIR,\"/relevant_sentences_all.json\"), \"w\") as f:\n",
    "  json.dump(filtered_sentences, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "ins",
   "language": "python",
   "name": "ins"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "270.545px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
